<!DOCTYPE html>
<html lang="de">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover"/>
  <title>PhO-Compress Atom-LLM — O(M) Lineares Modell (FINAL, IPA lokal aus ./data/*.txt)</title>

  <!-- TensorFlow.js + WebGPU backend -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.22.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgpu@4.22.0/dist/tf-backend-webgpu.min.js"></script>

  <!-- Tailwind (CDN, unminified dev CDN ist ok laut Vorgabe) -->
  <script src="https://cdn.tailwindcss.com"></script>

  <style>
    body{background:#f5f6f8;color:#1f2937;font-family:ui-sans-serif,-apple-system,Segoe UI,Roboto,Helvetica,Arial}
    .container{max-width:1100px;margin:32px auto;padding:0 16px}
    .card{background:#fff;border:1px solid #e5e7eb;border-radius:14px;box-shadow:0 1px 2px rgba(0,0,0,.04);padding:18px;margin:16px 0}
    .row{display:grid;gap:12px}
    @media(min-width:1000px){.row{grid-template-columns:1fr 1fr}}
    textarea,input,button,select{font-family:inherit}
    textarea{width:100%;min-height:140px;border:1px solid #d1d5db;border-radius:10px;padding:10px}
    input,select{width:100%;border:1px solid #d1d5db;border-radius:10px;padding:8px}
    label{font-size:12px;color:#6b7280;margin-bottom:4px;display:block}
    .btn{background:#4f46e5;border:none;color:#fff;border-radius:10px;padding:10px 14px;font-weight:600;cursor:pointer}
    .btn.secondary{background:#059669}
    .btn.grey{background:#6b7280}
    .btn.warn{background:#f59e0b}
    .btn.red{background:#ef4444}
    .btn:disabled{opacity:.5;cursor:not-allowed}
    .log{height:220px;overflow:auto;background:#0b1020;color:#d1d5db;border-radius:10px;padding:10px;font-family:ui-monospace,Menlo,Consolas,monospace;font-size:12px}
    .mono{font-family:ui-monospace,Menlo,Consolas,monospace}
    .muted{color:#6b7280}
    .spinner{border:4px solid rgba(0,0,0,.1);border-left-color:#4f46e5;border-radius:50%;width:20px;height:20px;animation:spin .8s linear infinite;display:inline-block}
    @keyframes spin{to{transform:rotate(360deg)}}
    .kpi{display:flex;gap:10px;flex-wrap:wrap}
    .kpi div{background:#f9fafb;border:1px solid #e5e7eb;border-radius:10px;padding:8px 10px}
    .row-4{display:grid;grid-template-columns:repeat(4,1fr);gap:12px}
  </style>
</head>
<body>
<div class="container">
  <h1 class="text-2xl font-extrabold mb-1">PhO-Compress Atom-LLM ($O(M)$ Lineare Skalierung)</h1>
  <div class="muted mb-4">IPA aus lokalen Dateien (./data/*.txt), Datei-Export/Import des Modells, kein IndexedDB, keine Heuristik-G2P.</div>

  <div class="card">
    <h3 class="font-bold text-lg mb-2">1) Korpus & G2P → IPA (Wörterbücher)</h3>
    <textarea id="txt" placeholder="Füge einen Textkorpus (Deutsch/Englisch) ein …"></textarea>

    <div class="row" style="margin-top:10px">
      <div>
        <label>Atom-Vokabular (Ziel)</label>
        <input id="atomVocab" type="number" min="64" max="1024" step="32" value="512"/>
      </div>
      <div>
        <label>Max. Kontext M (Lineares Modell)</label>
        <input id="seqLen" type="number" min="64" max="768" step="32" value="128"/>
      </div>
    </div>

    <div class="row" style="margin-top:10px">
      <div class="flex items-center gap-3">
        <label>Sprache G2P</label>
        <select id="g2pLang">
          <option value="de_DE" selected>Deutsch (de_DE)</option>
          <option value="en_US">Englisch (en_US)</option>
        </select>
        <button id="btnBuildIPA" class="btn grey">1. G2P → IPA erzeugen</button>
        <button id="btnTrainAtoms" class="btn warn" disabled>2. Atome lernen (BPE auf IPA)</button>
      </div>
      <div class="flex items-center gap-3">
        <label class="muted">Zusatz-Wörterbuch (optional, TXT <wort>\t<ipa> oder JSON {"wort":"ipa"}):</label>
        <input type="file" id="dictFile" accept=".txt,application/json"/>
        <button id="btnClearDict" class="btn red">Zusatz-Wörterbuch leeren</button>
      </div>
    </div>

    <div class="kpi mt-3">
      <div id="kpiIPA" class="muted">IPA: –</div>
      <div id="kpiAtoms" class="muted">Atome (M): –</div>
      <div id="kpiComp" class="muted">Kompression: –</div>
      <div id="kpiDict" class="muted">Wörterbücher: laden …</div>
    </div>
  </div>

  <div class="card">
    <h3 class="font-bold text-lg mb-2">2) Lineares Modell Setup & Training</h3>
    <div class="row-4">
      <div><label>Schichten L</label><input id="nLayers" type="number" min="1" max="8" value="3"/></div>
      <div><label>d_model</label><input id="dModel" type="number" min="64" max="512" step="32" value="256"/></div>
      <div><label>FFN-Dim</label><input id="dFF" type="number" min="128" max="2048" step="64" value="512"/></div>
      <div><label>Epochen</label><input id="epochs" type="number" min="1" max="40" value="4"/></div>
    </div>

    <div class="row-4 mt-3">
      <div><label>LR (Adam)</label><input id="lr" type="number" min="0.00005" max="0.02" step="0.00005" value="0.002"/></div>
      <div><label>Batchgröße</label><input id="batch" type="number" min="8" max="128" step="8" value="32"/></div>
      <div><label>Steps/Epoche (0=auto)</label><input id="stepCap" type="number" min="0" max="50000" step="100" value="400"/></div>
      <div class="flex items-end gap-3">
        <label class="muted"><input id="useGPU" type="checkbox" checked/> WebGPU bevorzugen</label>
        <label class="muted"><input id="useDropout" type="checkbox"/> Dropout 0.1</label>
        <label class="muted"><input id="autoLR" type="checkbox" checked/> Auto-LR</label>
      </div>
    </div>

    <div class="flex items-center gap-3 mt-3">
      <button id="btnTrain" class="btn" disabled>
        <span id="trainText">3. Training starten</span>
        <span id="spin" class="spinner" style="display:none;margin-left:8px"></span>
      </button>
      <button id="btnStop" class="btn red" disabled>Stop</button>

      <!-- Datei-basierter Export/Import -->
      <button id="btnExport" class="btn warn" disabled>Modell exportieren (Datei)</button>
      <label class="btn secondary" for="fileImport" style="cursor:pointer">Modell importieren (Datei)</label>
      <input id="fileImport" type="file" accept="application/json" style="display:none"/>

      <div id="stats" class="muted"></div>
    </div>
  </div>

  <div class="card">
    <h3 class="font-bold text-lg mb-2">3) Inferenz (Atom → IPA)</h3>
    <textarea id="prompt" placeholder="Prompt (Text) eingeben …" disabled></textarea>
    <div class="row">
      <div><label>Sampling-Modus</label>
        <select id="sampleMode">
          <option value="topp" selected>Top-p</option>
          <option value="topk">Top-k</option>
        </select>
      </div>
      <div><label>Top-k</label><input id="topK" type="number" min="1" max="200" value="64" disabled/></div>
      <div><label>Top-p</label><input id="topP" type="number" min="0.05" max="1.0" step="0.05" value="0.9"/></div>
      <div><label>Temperatur</label><input id="temp" type="number" min="0.1" max="2.0" step="0.1" value="0.9"/></div>
    </div>
    <div class="row mt-2">
      <div><label>Tokens generieren</label><input id="nGen" type="number" min="1" max="2000" value="200"/></div>
      <div class="flex items-end gap-3">
        <button id="btnGen" class="btn secondary" disabled>Generieren</button>
        <button id="btnStopGen" class="btn red" disabled>Stop</button>
        <div id="inferStats" class="muted"></div>
      </div>
    </div>
    <div class="mt-3">
      <div class="muted mb-1">Ausgabe (IPA)</div>
      <div id="outIPA" class="mono" style="white-space:pre-wrap;background:#f9fafb;border:1px solid #e5e7eb;border-radius:10px;padding:10px;min-height:80px"></div>
    </div>
  </div>

  <div class="card">
    <h3 class="font-bold text-lg mb-2">Perplexity & Log</h3>
    <div id="metrics" class="muted mb-2"></div>
    <div id="log" class="log">[Log bereit. Lineares $O(M)$-Modell aktiviert.]</div>
  </div>
</div>

<script>
/* =================== Core Helpers =================== */
const $ = id => document.getElementById(id);
const MAX_LOG_CHARS = 120000;
const BOS_ID = 0, EOS_ID = 1;

function logLine(msg){
  const el = $('log');
  const line = `[${new Date().toLocaleTimeString()}] ${msg}\n`;
  el.textContent = line + el.textContent;
  if (el.textContent.length > MAX_LOG_CHARS) el.textContent = el.textContent.slice(0, MAX_LOG_CHARS);
}
function clamp(v, lo, hi){ v = Number.isFinite(v) ? v : lo; return Math.max(lo, Math.min(hi, v)); }
function valNum(id, fallback, lo=null, hi=null){
  const el = $(id);
  let v = Number(el?.value);
  if(!Number.isFinite(v)) v = fallback;
  if(lo!=null) v = Math.max(lo, v);
  if(hi!=null) v = Math.min(hi, v);
  return v;
}
function valBool(id, fallback=false){ return $(id)?.checked ?? fallback; }
function setButtonState(id, disabled, text, showSpinner=false) {
  const btn = $(id);
  if (!btn) return;
  btn.disabled = !!disabled;
  if (id === 'btnTrain') {
    const labelEl = btn.querySelector('span#trainText');
    if (labelEl && text != null) labelEl.textContent = String(text);
    const spinEl = btn.querySelector('span#spin');
    if (spinEl) spinEl.style.display = showSpinner ? 'inline-block' : 'none';
  } else {
    const span = btn.querySelector('span');
    if (span && text != null) span.textContent = String(text);
    else if (text != null) btn.textContent = String(text);
  }
}
async function microYield(){
  await new Promise(r=>setTimeout(r,0));
  await tf.nextFrame();
}

/* =================== IPA Wörterbücher – lokal laden =================== */
/* Format laut ipa-dict: Tab-getrennt: [ENTRY][TAB][IPA], UTF-8. Quelle: README. */
let EN_DICT = new Map(); // en_US / en_EN (lokal)
let DE_DICT = new Map(); // de_DE (lokal)
let EXTRA_DICT = new Map(); // Benutzer-Overrides

function dictInfo(){
  $('kpiDict').textContent =
    `Wörterbücher: en=${EN_DICT.size} • de=${DE_DICT.size} • Zusatz=${EXTRA_DICT.size}`;
}

/* Parser für TXT (tab-delimited) */
function parseTxtDict(text){
  const map = new Map();
  const lines = (text||'').split(/\r?\n/);
  for (let line of lines){
    if (!line) continue;
    if (line.startsWith('#')) continue;
    const parts = line.split('\t');
    if (parts.length < 2) continue;
    const key = String(parts[0]).trim().toLowerCase();
    const ipa = String(parts.slice(1).join('\t')).trim();
    if (!key || !ipa) continue;
    map.set(key, ipa);
  }
  return map;
}

/* Parser für JSON (entweder {"wort":"ipa"} oder ipa-dict-ähnliche Struktur) */
function parseJsonDict(jsonObj){
  const map = new Map();
  if (!jsonObj || typeof jsonObj !== 'object') return map;
  // flach: {"word":"ipa"}
  let flat = jsonObj;
  // ipa-dict JSON kann {"LANG":[ { "word":"ipa", ... } ]} sein -> in flache Map umwandeln
  if (Object.keys(jsonObj).length===1 && Array.isArray(Object.values(jsonObj)[0])){
    const arr = Object.values(jsonObj)[0];
    for (const obj of arr){
      if (!obj || typeof obj!=='object') continue;
      for (const [k,v] of Object.entries(obj)){
        if (k && v) map.set(String(k).toLowerCase(), String(v));
      }
    }
    return map;
  }
  for (const [k,v] of Object.entries(flat)){
    if (k && v) map.set(String(k).toLowerCase(), String(v));
  }
  return map;
}

async function fetchText(url){
  const r = await fetch(url, {mode:'same-origin'});
  if(!r.ok) throw new Error(r.status+' '+r.statusText);
  return await r.text();
}
async function fetchJSON(url){
  const r = await fetch(url, {mode:'same-origin'});
  if(!r.ok) throw new Error(r.status+' '+r.statusText);
  return await r.json();
}

/* Lädt lokal verfügbare Dateien (gleicher Origin), ohne CORS-Fehler.
   Reihenfolge: en_EN.txt → en_US.txt → en_US.json ; de_DE.txt → de_DE.json */
async function tryLoadDictsLocal(){
  let enLoaded = false, deLoaded = false;

  // Englisch
  const enCandidates = [
    './data/en_EN.txt', // Wunschpfad
    './data/en_US.txt',
    './data/en_US.json'
  ];
  for (const url of enCandidates){
    try{
      if (url.endsWith('.txt')){
        const txt = await fetchText(url);
        const m = parseTxtDict(txt);
        if (m.size>0){ EN_DICT = m; enLoaded = true; logLine(`Englisches IPA-WB geladen: ${url} (${m.size} Einträge)`); break; }
      } else {
        const obj = await fetchJSON(url);
        const m = parseJsonDict(obj);
        if (m.size>0){ EN_DICT = m; enLoaded = true; logLine(`Englisches IPA-WB geladen: ${url} (${m.size} Einträge)`); break; }
      }
    } catch(e){
      // still try next
    }
  }

  // Deutsch
  const deCandidates = [
    './data/de_DE.txt',
    './data/de_DE.json'
  ];
  for (const url of deCandidates){
    try{
      if (url.endsWith('.txt')){
        const txt = await fetchText(url);
        const m = parseTxtDict(txt);
        if (m.size>0){ DE_DICT = m; deLoaded = true; logLine(`Deutsches IPA-WB geladen: ${url} (${m.size} Einträge)`); break; }
      } else {
        const obj = await fetchJSON(url);
        const m = parseJsonDict(obj);
        if (m.size>0){ DE_DICT = m; deLoaded = true; logLine(`Deutsches IPA-WB geladen: ${url} (${m.size} Einträge)`); break; }
      }
    } catch(e){
      // still try next
    }
  }

  dictInfo();
  if (!enLoaded) logLine('Hinweis: Kein englisches IPA-Wörterbuch unter ./data/en_EN.txt|en_US.txt|en_US.json gefunden.');
  if (!deLoaded) logLine('Hinweis: Kein deutsches IPA-Wörterbuch unter ./data/de_DE.txt|de_DE.json gefunden.');
}

/* Zusatz-Wörterbuch importieren (txt oder json) */
function mergeExtraDictFromTxt(text){
  const m = parseTxtDict(text);
  let added=0; for (const [k,v] of m){ EXTRA_DICT.set(k,v); added++; }
  dictInfo(); logLine(`Zusatz-Wörterbuch (TXT): +${added} Einträge.`);
}
function mergeExtraDictFromJson(obj){
  const m = parseJsonDict(obj);
  let added=0; for (const [k,v] of m){ EXTRA_DICT.set(k,v); added++; }
  dictInfo(); logLine(`Zusatz-Wörterbuch (JSON): +${added} Einträge.`);
}

/* Tokenizer (reine Wort-/Zeichen-Separierung, kein Heuristik-G2P) */
const WORD_RE = /[\p{L}\p{M}\p{Nd}]+/u;
function tokenize(text){
  const m = text.normalize('NFC').match(/[\p{L}\p{M}\p{Nd}]+|[^\s]/gu) || [];
  return m.map(s=>s.toString());
}

function g2pConvert(text){
  const lang = $('g2pLang').value; // 'de_DE' oder 'en_US'
  const BASE = (lang==='de_DE') ? DE_DICT : EN_DICT;
  if (BASE.size===0 && EXTRA_DICT.size===0){
    throw new Error('IPA-Wörterbuch nicht verfügbar (lokale Dateien fehlen und kein Zusatz-WB).');
  }
  const toks = tokenize(text.toLowerCase());
  const out = [];
  for (const t of toks){
    if (!WORD_RE.test(t)){ out.push(t); continue; } // Satzzeichen etc.
    const hit = EXTRA_DICT.get(t) ?? BASE.get(t);
    if (hit){ out.push(hit); }
    else {
      // bewusst KEINE Regel-Heuristik – unbekannte Wörter klar markieren
      out.push(`‹${t}›`);
    }
  }
  return out.join(' ').replace(/\s+/g,' ').trim();
}

/* =================== Atomizer (BPE) =================== */
function strToCodepoints(str){ const arr = []; for (let i=0;i<str.length;i++) arr.push(str.charCodeAt(i)); return arr; }
function codepointsToStr(arr){ return String.fromCharCode(...arr); }

function buildAtomBPE(rawIPA, targetVocab=512){
  return new Promise(async (resolve) => {
    const cps = strToCodepoints(rawIPA);
    const uniq = Array.from(new Set(cps.filter(x=>!Number.isNaN(x)))).sort((a,b)=>a-b);
    const cp2id = new Map(); const id2cp = new Map();
    let nextId = 2; // 0 BOS, 1 EOS reserviert
    for (const c of uniq){ cp2id.set(c,nextId); id2cp.set(nextId,c); nextId++; }
    let seq = cps.map(c => cp2id.get(c) ?? BOS_ID);

    const merges = [];
    const baseVocab = nextId;

    function countPairs(sequence){
      const counts = new Map();
      for (let i=0;i<sequence.length-1;i++){
        const key = sequence[i] + ',' + sequence[i+1];
        counts.set(key, (counts.get(key)||0) + 1);
      }
      return counts;
    }
    function applyBestPair(sequence, a, b, newId){
      const out = [];
      for (let i=0;i<sequence.length;){
        if (i<sequence.length-1 && sequence[i]===a && sequence[i+1]===b){ out.push(newId); i+=2; }
        else { out.push(sequence[i]); i+=1; }
      }
      return out;
    }

    const target = Math.max(baseVocab+10, Math.min(targetVocab, 4096));
    for (let step=0; step<(target - baseVocab); step++){
      if ((step % 32) === 0) {
        $('inferStats').textContent = `BPE: ${step+1}/${target-baseVocab}`;
        await microYield();
      }
      const counts = countPairs(seq);
      if (counts.size===0) break;
      let bestKey=null, bestCnt=0;
      for (const [k,c] of counts){ if (c>bestCnt){ bestCnt=c; bestKey=k; } }
      if (!bestKey) break;
      const [aStr,bStr] = bestKey.split(',');
      const a=parseInt(aStr,10), b=parseInt(bStr,10);
      const id = nextId++;
      merges.push({a,b,id});
      seq = applyBestPair(seq, a, b, id);
      if ((step % 128) === 0) logLine(`BPE merge ${step+1}: (${a},${b}) → ${id} (count=${bestCnt})`);
    }

    function encodeIPAtoAtoms(str, addBos=false, addEos=false){
      const seq0 = strToCodepoints(str).map(c => cp2id.get(c));
      let tokens = seq0.filter(v=>Number.isFinite(v));
      for (const m of merges){
        const {a,b,id} = m;
        const out = [];
        for (let i=0;i<tokens.length;){
          if (i<tokens.length-1 && tokens[i]===a && tokens[i+1]===b){ out.push(id); i+=2; }
          else { out.push(tokens[i]); i++; }
        }
        tokens = out;
      }
      if (addBos) tokens = [BOS_ID, ...tokens];
      if (addEos) tokens = [...tokens, EOS_ID];
      return tokens;
    }
    function decodeAtomsToIPA(tokens){
      const id2pair = new Map(merges.map(m=>[m.id,[m.a,m.b]]));
      function expand(id){
        if (id===BOS_ID || id===EOS_ID) return [];
        if (id2cp.has(id)) return [ id2cp.get(id) ];
        const pr = id2pair.get(id);
        if (!pr) return [];
        return expand(pr[0]).concat(expand(pr[1]));
      }
      const cpsOut = [];
      for (const t of tokens){
        if (t===BOS_ID || t===EOS_ID) continue;
        if (id2cp.has(t)) cpsOut.push(id2cp.get(t));
        else cpsOut.push(...expand(t));
      }
      return codepointsToStr(cpsOut);
    }

    const vocabSize = nextId;
    resolve({ encodeIPAtoAtoms, decodeAtomsToIPA, vocabSize, merges, cp2id, id2cp });
  });
}

/* =================== Modell (O(M) mit kausaler Conv + Gate + MLP) =================== */
function rmsNorm(x, gamma, eps=1e-5){
  const meanSq = tf.mean(tf.square(x), -1, true);
  const xhat = tf.div(x, tf.sqrt(tf.add(meanSq, eps)));
  return tf.mul(xhat, gamma);
}
function sinusoidalPositionalEncoding(T, d){
  const pos = tf.range(0, T, 1, 'float32');
  const half = Math.floor(d/2);
  const i2 = tf.range(0, half, 1, 'float32');
  const div = tf.exp(tf.mul(i2, tf.scalar(-Math.log(10000.0)/(Math.max(1,half-1)))));
  const ang = pos.reshape([T,1]).matMul(div.reshape([1,half]));
  const sin = tf.sin(ang), cos=tf.cos(ang);
  let pe = tf.concat([sin, cos], -1);
  if(2*half < d){ const pad = tf.zeros([T, d-2*half]); pe = tf.concat([pe, pad], -1); }
  return pe;
}
function softmaxCEfromLogits(logits, targets){
  const [B,T,V] = logits.shape;
  const oneHot = tf.oneHot(targets.toInt(), V).reshape([B*T, V]);
  const logits2d = logits.reshape([B*T, V]);
  const lossPer = tf.losses.softmaxCrossEntropy(oneHot, logits2d);
  return tf.mean(lossPer);
}

function buildModel(cfg){
  const { V, dModel, dFF, nLayers, T, weightTying=true, dropoutTrain } = cfg;

  const E = tf.variable(tf.randomNormal([V, dModel], 0, 0.02, 'float32'), true, 'E');
  const pe = sinusoidalPositionalEncoding(T, dModel);

  const layers = [];
  const CONV_KERNEL_SIZE = 5;
  for(let l=0;l<nLayers;l++){
    layers.push({
      g1: tf.variable(tf.ones([dModel]), true, `L${l}_g1`),
      g2: tf.variable(tf.ones([dModel]), true, `L${l}_g2`),

      W_Conv: tf.variable(tf.randomNormal([CONV_KERNEL_SIZE, dModel, dModel], 0, Math.sqrt(2/(dModel+dModel))), true, `L${l}_W_Conv`),
      W_Gate: tf.variable(tf.randomNormal([dModel, dModel], 0, Math.sqrt(2/(dModel+dModel))), true, `L${l}_W_Gate`),
      b_Gate: tf.variable(tf.zeros([dModel]), true, `L${l}_b_Gate`),

      W1: tf.variable(tf.randomNormal([dModel, dFF], 0, Math.sqrt(2/(dModel+dFF))), true, `L${l}_W1`),
      b1: tf.variable(tf.zeros([dFF]), true, `L${l}_b1`),
      W2: tf.variable(tf.randomNormal([dFF, dModel], 0, Math.sqrt(2/(dFF+dModel))), true, `L${l}_W2`),
      b2: tf.variable(tf.zeros([dModel]), true, `L${l}_b2`),
    });
  }

  const bout = tf.variable(tf.zeros([V]), true, 'bout');
  const W_out = tf.variable(tf.randomNormal([dModel, V], 0, 0.02, 'float32'), true, 'W_out');

  const params = {E, layers, bout, pe, cfg, W_out, CONV_KERNEL_SIZE};

  function dropout(x, rate){
    if(!dropoutTrain || rate<=0) return x;
    const keep = 1-rate;
    const m = tf.randomUniform(x.shape,'float32').greater(tf.scalar(rate)).toFloat();
    return tf.mul(x, tf.div(m, tf.scalar(keep)));
  }

  function forward(ids, training=true){
    return tf.tidy(() => {
      const B = ids.shape[0], Tcur = ids.shape[1], dM = params.cfg.dModel;

      const emb = tf.gather(params.E, ids.flatten()).reshape([B,Tcur,dM]);
      let x = emb.add(params.pe.slice([0,0],[Tcur,dM]).reshape([1,Tcur,dM]));
      if (params.cfg.dropoutTrain && training) x = dropout(x, 0.1);

      for(const L of params.layers){
        let h = rmsNorm(x, L.g1);

        // Kausale Conv1D (links gepaddet)
        const K = params.CONV_KERNEL_SIZE;
        const paddingSize = K - 1;
        const zeroPadding = tf.zeros([B, paddingSize, dM]);
        const paddedH = tf.concat([zeroPadding, h], 1);

        let conv_out_full = tf.conv1d(paddedH, L.W_Conv, 1, 'valid');
        let conv_out = conv_out_full.slice([0, 0, 0], [B, Tcur, dM]);

        const gate_proj = conv_out.reshape([-1, dM]).matMul(L.W_Gate).add(L.b_Gate);
        const gate = tf.sigmoid(gate_proj).reshape([B, Tcur, dM]);

        const linear_out = tf.mul(conv_out, gate);
        x = x.add((params.cfg.dropoutTrain && training) ? dropout(linear_out, 0.1) : linear_out);

        // MLP
        let h2 = rmsNorm(x, L.g2);
        const mlp2 = h2.reshape([-1, dM]).matMul(L.W1).add(L.b1);
        const silu = tf.mul(mlp2, tf.sigmoid(mlp2));
        const mlp_out = silu.matMul(L.W2).add(L.b2).reshape([B, Tcur, dM]);
        x = x.add((params.cfg.dropoutTrain && training) ? dropout(mlp_out, 0.1) : mlp_out);
      }

      const x_2d = x.reshape([-1, dM]);
      let logits_2d = params.cfg.weightTying ? x_2d.matMul(params.E.transpose()) : x_2d.matMul(params.W_out);
      const logits = logits_2d.reshape([B, Tcur, params.cfg.V]);
      return logits.add(params.bout);
    });
  }

  function variables(){
    const vs = [E, bout];
    for (const L of params.layers){
      vs.push(L.g1, L.g2, L.W_Conv, L.W_Gate, L.b_Gate, L.W1, L.b1, L.W2, L.b2);
    }
    vs.push(W_out);
    return vs;
  }
  return {params, forward, variables};
}

/* =================== Backend =================== */
async function setupBackend(preferGPU=true){
  try {
    if (preferGPU) {
      await tf.setBackend('webgpu');
      await tf.ready();
      logLine(`Backend: ${tf.getBackend()} (WebGPU aktiv)`);
      return;
    }
    throw new Error('Skip WebGPU');
  } catch(e){
    logLine(`WebGPU nicht aktiv (${e?.message||e}). Versuche WebGL/CPU...`);
  }
  try {
    await tf.setBackend('webgl');
    await tf.ready();
    logLine(`Backend: ${tf.getBackend()} (WebGL aktiv)`);
  } catch(e){
    await tf.setBackend('cpu');
    await tf.ready();
    logLine(`Backend: ${tf.getBackend()} (CPU aktiv)`);
  }
}

/* =================== Datenaufbereitung & Training =================== */
let MODEL=null, ATOMIZER=null, TOKENS=null, IPA_CORPUS='';
let CANCEL_TRAIN=false, CANCEL_GEN=false;
let modelRev = 0;

function buildStarts(tokens, T, valPct){
  const N = tokens.length;
  const usable = Math.max(0, N - (T+1));
  const all = new Uint32Array(usable);
  for (let i=0;i<usable;i++) all[i]=i;
  const valN = Math.floor(usable*(valPct/100));
  const trainN = usable - valN;
  return {train: all.slice(0,trainN), val: all.slice(trainN)};
}
function sampleBatch(starts, tokens, B, T){
  const X = new Int32Array(B*T), Y = new Int32Array(B*T);
  const n = starts.length;
  if (n===0) return {X,Y};
  for (let b=0;b<B;b++){
    const s = starts[(Math.random()*n)|0];
    for (let t=0;t<T;t++){
      X[b*T+t] = tokens[s+t];
      Y[b*T+t] = tokens[s+t+1];
    }
  }
  return {X,Y};
}

async function train(){
  await setupBackend(valBool('useGPU', true));
  const raw = ($('txt')?.value ?? '');
  if(!raw.trim()){ logLine('Fehler: Kein Korpus.'); return; }
  if(!ATOMIZER || !TOKENS){ logLine('Fehler: Bitte zuerst Atome lernen.'); return; }

  setButtonState('btnTrain', true, 'Trainiere…', true);
  $('btnStop').disabled = false;
  CANCEL_TRAIN = false;

  try {
    const T = valNum('seqLen', 128, 64, 768), epochs = valNum('epochs', 4), B = valNum('batch', 32);
    let lr = valNum('lr', 0.002);
    const stepCap = valNum('stepCap', 400), valSplit = 10, autoLR = valBool('autoLR', true);
    const L = valNum('nLayers', 3), dModel = valNum('dModel', 256), dFF = valNum('dFF', 512);
    const weightTying = true, dropoutTrain = valBool('useDropout', false);
    const V = ATOMIZER.vocabSize;

    if (MODEL) MODEL.variables().forEach(v => v.dispose());
    MODEL = buildModel({V, dModel, dFF, nLayers: L, T, weightTying, dropoutTrain});

    let optim = tf.train.adam(lr, 0.9, 0.999, 1e-8);

    const {train:trainStarts, val:valStarts} = buildStarts(TOKENS, T, valSplit);
    const rawSteps = Math.max(1, Math.floor(trainStarts.length / B));
    const stepsPerEpoch = stepCap>0 ? Math.min(stepCap, rawSteps) : rawSteps;

    $('stats').textContent = `V=${V} • M=${T} • B=${B} • L=${L} • d=${dModel} • dFF=${dFF} • Steps/Ep=${stepsPerEpoch}`;

    let bestPPL=Infinity, bad=0;
    const myModelRev = ++modelRev;

    for (let ep=0; ep<epochs; ep++){
      if (myModelRev !== modelRev || CANCEL_TRAIN) break;
      logLine(`Starte Epoche ${ep+1}/${epochs} mit LR=${lr.toFixed(6)}`);
      const t0 = performance.now();
      let lossAcc=0;

      for (let s=0;s<stepsPerEpoch;s++){
        if (myModelRev !== modelRev || CANCEL_TRAIN) break;
        const {X,Y} = sampleBatch(trainStarts, TOKENS, B, T);

        const lossScalar = optim.minimize(() => {
          const x = tf.tensor2d(X, [B,T], 'int32');
          const y = tf.tensor2d(Y, [B,T], 'int32');
          const logits = MODEL.forward(x, true);
          const loss = softmaxCEfromLogits(logits, y);
          return loss;
        }, true, MODEL.variables());

        const lossArr = await lossScalar.data();
        const lossVal = lossArr[0];
        lossScalar.dispose();
        if (!Number.isFinite(lossVal)) { logLine(`FEHLER: Loss=${lossVal}. Abbruch.`); throw new Error('NaN/Inf'); }
        lossAcc += lossVal;

        if ((s % 200) === 0) logLine(`Ep ${ep+1} Step ${s}/${stepsPerEpoch} Loss=${lossVal.toFixed(4)}`);
        if ((s % 512) === 0) { const m=tf.memory(); logLine(`mem: tensors=${m.numTensors} bytes=${m.numBytes}`); }
        await tf.nextFrame();
      }
      if (myModelRev !== modelRev || CANCEL_TRAIN) break;

      const t1 = performance.now();
      let nllSum = 0, tokCount = 0;
      const evalB = Math.min(B, 32);
      const evalSteps = Math.min(10, Math.max(0, Math.floor(valStarts.length / Math.max(1, evalB))));

      for (let es=0; es<evalSteps; es++){
        if (myModelRev !== modelRev || CANCEL_TRAIN) break;
        const {X,Y} = sampleBatch(valStarts, TOKENS, evalB, T);
        const valLoss = tf.tidy(() => {
          const x = tf.tensor2d(X, [evalB,T], 'int32');
          const y = tf.tensor2d(Y, [evalB,T], 'int32');
          return softmaxCEfromLogits(MODEL.forward(x, false), y);
        });
        const v = (await valLoss.data())[0];
        valLoss.dispose();
        nllSum += v * (evalB*T);
        tokCount += (evalB*T);
        await tf.nextFrame();
      }
      if (myModelRev !== modelRev || CANCEL_TRAIN) break;

      const valPPL = Math.exp(nllSum/Math.max(1,tokCount));
      const tokps = (stepsPerEpoch * B * T) / Math.max(0.001, (t1-t0)/1000);
      $('metrics').innerHTML =
        `<b>Epoche ${ep+1}/${epochs}</b> &nbsp; Loss=${(lossAcc/stepsPerEpoch).toFixed(4)} &nbsp; Val PPL=${valPPL.toFixed(2)} &nbsp; ~Tok/s=${tokps.toFixed(0)} &nbsp; LR=${lr.toFixed(5)}`;
      logLine(`Ep ${ep+1}: loss=${(lossAcc/stepsPerEpoch).toFixed(4)} | ValPPL=${valPPL.toFixed(2)} | ${(t1-t0).toFixed(0)}ms | ~${tokps.toFixed(0)} tok/s`);

      if (valBool('autoLR', true)){
        if (valPPL + 0.01 < bestPPL){ bestPPL=valPPL; bad=0; }
        else {
          bad++;
          if (bad>=2 && lr>1e-4){
            lr = Math.max(1e-4, lr*0.5);
            if (typeof optim.setLearningRate === 'function') {
              optim.setLearningRate(lr);
            } else {
              optim.dispose?.();
              optim = tf.train.adam(lr, 0.9, 0.999, 1e-8);
            }
            bad=0; logLine(`Auto-LR → ${lr.toFixed(6)}`);
          }
        }
      }
      await tf.nextFrame();
    }
    if (myModelRev === modelRev && !CANCEL_TRAIN) logLine('Training abgeschlossen. Modell ist bereit zur Inferenz.');
    $('prompt').disabled = false; $('btnGen').disabled = false; $('btnExport').disabled = false;
  } catch(e){
    console.error(e); logLine('Fehler: '+(e?.message||e));
  } finally {
    setButtonState('btnTrain', false, '3. Training starten', false);
    $('btnStop').disabled = true;
    CANCEL_TRAIN=false;
  }
}

/* =================== Sampling & Inferenz =================== */
function sampleFromProbs(probs, mode, topK, topP){
  const order = Array.from(probs.keys()).sort((a,b)=>probs[b]-probs[a]);
  if (mode==='topk'){
    const K = Math.max(1, Math.min(topK, probs.length));
    const keep = order.slice(0,K);
    let s=0; for(const i of keep) s+=probs[i];
    let r=Math.random()*s;
    for(const i of keep){ r-=probs[i]; if(r<=0) return i; }
    return keep[keep.length-1];
  } else {
    const pth = clamp(topP, 0.05, 1.0);
    let cum=0, keep=[];
    for(const i of order){ keep.push(i); cum+=probs[i]; if(cum>=pth) break; }
    let s=0; for(const i of keep) s+=probs[i];
    let r=Math.random()*s;
    for(const i of keep){ r-=probs[i]; if(r<=0) return i; }
    return keep[keep.length-1];
  }
}

async function generate(){
  if (!MODEL || !ATOMIZER){ logLine('Bitte zuerst Atomizer + Modell trainieren/laden.'); return; }

  $('btnGen').disabled = true; $('btnStopGen').disabled = false; $('inferStats').textContent = 'Generiere...';
  CANCEL_GEN = false;

  const mode = $('sampleMode').value, topK = valNum('topK', 64), topP = valNum('topP', 0.9), temp = valNum('temp', 0.9);
  const nGen = valNum('nGen', 200), T = MODEL.params.cfg.T;
  const prompt = ($('prompt')?.value ?? '').trim();
  if (!prompt){ logLine('Kein Prompt.'); $('btnGen').disabled = false; $('btnStopGen').disabled = true; return; }

  const promptIPA = g2pConvert(prompt);
  let ctx = ATOMIZER.encodeIPAtoAtoms(promptIPA, true, false);
  const t0 = performance.now();

  let outTokens = [];
  for(let step=0; step<nGen; step++){
    if (CANCEL_GEN) { logLine('Generierung abgebrochen.'); break; }
    const ctxSlice = ctx.slice(-T);
    const len=ctxSlice.length, padLen = T - len;
    const inpArr = (padLen>0) ? (new Int32Array([...Array(padLen).fill(BOS_ID), ...ctxSlice])) : new Int32Array(ctxSlice);

    const input = tf.tensor2d(inpArr,[1,T],'int32');
    const logits = MODEL.forward(input, false);
    const lastTensor = logits.slice([0,T-1,0],[1,1,MODEL.params.cfg.V]).reshape([MODEL.params.cfg.V]);
    const last = await lastTensor.data();
    lastTensor.dispose(); logits.dispose(); input.dispose();

    let maxv = -Infinity; for (let i=0;i<last.length;i++) if (last[i]>maxv) maxv=last[i];
    const probs = new Float32Array(last.length);
    let Z=0; const invT = 1/clamp(temp,0.1,2.0);
    for (let i=0;i<last.length;i++){ const e=Math.exp((last[i]-maxv)*invT); probs[i]=e; Z+=e; }
    for (let i=0;i<probs.length;i++) probs[i]/=Z;

    const nxt = sampleFromProbs(probs, mode, topK, topP);
    if (nxt === EOS_ID) break;
    ctx.push(nxt); outTokens.push(nxt);

    if ((step % 8) === 0) await tf.nextFrame();
  }
  const t1 = performance.now();

  $('btnGen').disabled = false; $('btnStopGen').disabled = true;
  $('inferStats').textContent = `Tokens: ${outTokens.length} • Latenz ${(t1-t0).toFixed(1)} ms`;

  const genIPA = ATOMIZER.decodeAtomsToIPA(outTokens);
  $('outIPA').textContent = genIPA;
  logLine(`Generiert ${outTokens.length} Atome (${(t1-t0).toFixed(0)} ms).`);
}

/* =================== Datei-Export / -Import =================== */
function ab2b64(buf){
  const bytes = new Uint8Array(buf);
  let bin=''; for (let i=0;i<bytes.length;i++) bin += String.fromCharCode(bytes[i]);
  return btoa(bin);
}
function b642ab(b64){
  const bin = atob(b64);
  const bytes = new Uint8Array(bin.length);
  for (let i=0;i<bin.length;i++) bytes[i] = bin.charCodeAt(i);
  return bytes.buffer;
}
function downloadBlob(filename, blob){
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url; a.download = filename;
  document.body.appendChild(a); a.click(); a.remove();
  URL.revokeObjectURL(url);
}

async function exportModel(){
  if (!MODEL || !ATOMIZER){ logLine('Export: Kein Modell/Atomizer vorhanden.'); return; }
  setButtonState('btnExport', true, 'Exportiere …', false);
  try {
    const named = {};
    for (const v of MODEL.variables()) {
      if (!v.name) throw new Error('Variable ohne Namen.');
      if (named[v.name]) throw new Error('Duplikat: '+v.name);
      named[v.name]=v;
    }
    const {data, specs} = await tf.io.encodeWeights(named);

    const payload = {
      version: 1,
      cfg: MODEL.params.cfg,
      weightSpecs: specs,
      weightDataB64: ab2b64(data),
      atomizer: {
        merges: ATOMIZER.merges,
        cp2id: Object.fromEntries(ATOMIZER.cp2id),
        id2cp: Object.fromEntries(ATOMIZER.id2cp),
        vocabSize: ATOMIZER.vocabSize
      }
    };
    const blob = new Blob([JSON.stringify(payload)], {type:'application/json'});
    downloadBlob('pho-atom-llm-model.json', blob);
    logLine(`Export abgeschlossen: ${(data.byteLength/1024/1024).toFixed(2)} MiB`);
  } catch(e){
    logLine('Export-Fehler: '+(e?.message||e));
  } finally {
    setButtonState('btnExport', false, 'Modell exportieren (Datei)', false);
  }
}

async function importModel(file){
  if(!file){ logLine('Import: Keine Datei gewählt.'); return; }
  const reader = new FileReader();
  reader.onload = async () => {
    try {
      const obj = JSON.parse(reader.result);
      const cfg = obj.cfg;
      if (MODEL) MODEL.variables().forEach(v => v.dispose());
      MODEL = buildModel(cfg);

      const weightData = b642ab(obj.weightDataB64);
      const weightMap = await tf.io.decodeWeights(weightData, obj.weightSpecs);
      const byName = {}; for (const v of MODEL.variables()) byName[v.name] = v;
      for (const [name, tensor] of Object.entries(weightMap)) {
        if (byName[name]) { byName[name].assign(tensor); tensor.dispose(); }
        else tensor.dispose();
      }

      ATOMIZER = {
        merges: obj.atomizer.merges,
        cp2id: new Map(Object.entries(obj.atomizer.cp2id).map(([k, v]) => [Number(k), v])),
        id2cp: new Map(Object.entries(obj.atomizer.id2cp).map(([k, v]) => [Number(k), v])),
        vocabSize: obj.atomizer.vocabSize,
        encodeIPAtoAtoms: function(str, addBos, addEos) {
          const merges = this.merges;
          const cp2id = this.cp2id;
          let tokens = strToCodepoints(str).map(c => cp2id.get(c)).filter(v=>Number.isFinite(v));
          for (const m of merges){
            const {a,b,id} = m;
            const out = [];
            for (let i=0;i<tokens.length;){
              if (i<tokens.length-1 && tokens[i]===a && tokens[i+1]===b){ out.push(id); i+=2; }
              else { out.push(tokens[i]); i++; }
            }
            tokens = out;
          }
          if (addBos) tokens = [BOS_ID, ...tokens];
          if (addEos) tokens = [...tokens, EOS_ID];
          return tokens;
        },
        decodeAtomsToIPA: function(tokens) {
          const id2pair = new Map(this.merges.map(m=>[m.id,[m.a,m.b]]));
          const id2cp = this.id2cp;
          function expand(id){
            if (id===BOS_ID || id===EOS_ID) return [];
            if (id2cp.has(id)) return [ id2cp.get(id) ];
            const pr = id2pair.get(id);
            if (!pr) return [];
            return expand(pr[0]).concat(expand(pr[1]));
          }
          const cpsOut = [];
          for (const t of tokens){
            if (t===BOS_ID || t===EOS_ID) continue;
            if (id2cp.has(t)) cpsOut.push(id2cp.get(t));
            else cpsOut.push(...expand(t));
          }
          return codepointsToStr(cpsOut);
        }
      };

      modelRev++;
      $('prompt').disabled = false; $('btnGen').disabled = false; $('btnExport').disabled = false;
      logLine('Import abgeschlossen. Modell & Atomizer geladen.');
    } catch(e){
      logLine('Import-Fehler: '+(e?.message||e));
    }
  };
  reader.readAsText(file);
}

/* =================== UI Wiring =================== */
window.addEventListener('load', async ()=>{
  // Backend init
  setupBackend(valBool('useGPU', true));

  // Wörterbücher lokal laden (kein CORS)
  try {
    await tryLoadDictsLocal();
  } catch(e){
    logLine('Fehler beim Laden lokaler IPA-Wörterbücher: '+(e?.message||e));
  }

  $('btnStop').addEventListener('click', ()=>{ CANCEL_TRAIN=true; $('btnStop').disabled = true; logLine('Training gestoppt (am Epochen-/Step-Ende).'); });
  $('btnStopGen').addEventListener('click', ()=>{ CANCEL_GEN=true; $('btnStopGen').disabled = true; });

  $('dictFile').addEventListener('change', (e)=>{
    const file = e.target.files?.[0];
    if (!file) return;
    const fr = new FileReader();
    if (file.name.toLowerCase().endsWith('.txt')){
      fr.onload = ()=>{ try { mergeExtraDictFromTxt(fr.result); } catch(err){ logLine('Zusatz-WB (TXT) Fehler: '+err.message);} };
      fr.readAsText(file, 'utf-8');
    } else {
      fr.onload = ()=>{ try { mergeExtraDictFromJson(JSON.parse(fr.result)); } catch(err){ logLine('Zusatz-WB (JSON) Fehler: '+err.message);} };
      fr.readAsText(file, 'utf-8');
    }
  });
  $('btnClearDict').addEventListener('click', ()=>{ EXTRA_DICT = new Map(); dictInfo(); logLine('Zusatz-Wörterbuch geleert.'); });

  $('btnBuildIPA').addEventListener('click', ()=>{
    const raw=($('txt')?.value??'').trim();
    if(!raw){ logLine('Kein Text.'); return; }
    try{
      const ipa=g2pConvert(raw);
      IPA_CORPUS = ipa;
      $('kpiIPA').textContent = `IPA: ${ipa.length} Zeichen`;
      $('btnTrainAtoms').disabled = false;
      logLine('1. G2P → IPA fertig.');
    }catch(e){
      logLine('G2P-Fehler: '+(e?.message||e)+'. Lege bitte die Wortlisten in ./data/ ab.');
    }
  });

  $('btnTrainAtoms').addEventListener('click', async ()=>{
    const raw=($('txt')?.value??'').trim();
    if(!raw){ logLine('Kein Text.'); return; }
    let ipa;
    try { ipa=g2pConvert(raw); } catch(e){ logLine('G2P-Fehler: '+(e?.message||e)); return; }
    const atomVocab = valNum('atomVocab', 512);
    const T = valNum('seqLen', 128);

    $('btnTrainAtoms').disabled = true; $('btnBuildIPA').disabled = true;
    $('inferStats').textContent = '2. Atomizer wird trainiert...';

    try {
      const atomizer = await buildAtomBPE(ipa, atomVocab);
      const atomIds = atomizer.encodeIPAtoAtoms(ipa, true, true);
      ATOMIZER = atomizer;
      TOKENS = new Uint32Array(atomIds);

      const ipaLen = ipa.length;
      const atomCount = TOKENS.length;
      $('kpiAtoms').textContent = `Atome (M): ${atomCount} (inkl. BOS/EOS)`;
      const roughComp = (ipaLen>0) ? (ipaLen / atomCount).toFixed(2) : '–';
      $('kpiComp').textContent = `Kompression: ${roughComp}×`;

      logLine(`PhenomEncoder abgeschlossen. Vocab=${ATOMIZER.vocabSize}.`);
      $('btnTrain').disabled = false;
      $('inferStats').textContent = 'Atome fertig. Bereit zum Training.';
    } catch (e) {
      logLine(`Fehler beim Atomizer-Training: ${e.message}`);
    } finally {
      $('btnTrainAtoms').disabled = false; $('btnBuildIPA').disabled = false;
    }
  });

  $('btnTrain').addEventListener('click', train);
  $('btnGen').addEventListener('click', generate);

  $('sampleMode').addEventListener('change', ()=>{
    const m=$('sampleMode').value;
    $('topK').disabled = (m!=='topk'); $('topP').disabled=(m!=='topp');
  });

  $('btnExport').addEventListener('click', exportModel);
  $('fileImport').addEventListener('change', (e)=>importModel(e.target.files?.[0]));
});
</script>
</body>
</html>
