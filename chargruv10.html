<!DOCTYPE html>
<html lang="de">
<head>
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover"/>
	<title>PhO-Compress Atom-LLM — Linear Attention (O(M) Training) • tf.js • WebGPU</title>

	<!-- tf.js & WebGPU Backend (>= 4.22.0) -->
	<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.22.0/dist/tf.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgpu@4.22.0/dist/tf-backend-webgpu.min.js"></script>

	<!-- Tailwind für UI -->
	<script src="https://cdn.tailwindcss.com"></script>
	<style>
		body{background:#f5f6f8;color:#1f2937;font-family:ui-sans-serif,-apple-system,Segoe UI,Roboto,Helvetica,Arial}
		.container{max-width:1100px;margin:32px auto;padding:0 16px}
		.card{background:#fff;border:1px solid #e5e7eb;border-radius:14px;box-shadow:0 1px 2px rgba(0,0,0,.04);padding:18px;margin:16px 0}
		.row{display:grid;gap:12px}
		@media(min-width:1000px){.row{grid-template-columns:1fr 1fr}}
		textarea,input,button,select{font-family:inherit}
		textarea{width:100%;min-height:140px;border:1px solid #d1d5db;border-radius:10px;padding:10px}
		input,select{width:100%;border:1px solid #d1d5db;border-radius:10px;padding:8px}
		label{font-size:12px;color:#6b7280;margin-bottom:4px;display:block}
		.btn{background:#4f46e5;border:none;color:#fff;border-radius:10px;padding:10px 14px;font-weight:600;cursor:pointer}
		.btn.secondary{background:#059669}
		.btn.grey{background:#6b7280}
		.btn.warn{background:#f59e0b}
		.btn:disabled{opacity:.5;cursor:not-allowed}
		.log{height:200px;overflow:auto;background:#0b1020;color:#d1d5db;border-radius:10px;padding:10px;font-family:ui-monospace,Menlo,Consolas,monospace;font-size:12px}
		.mono{font-family:ui-monospace,Menlo,Consolas,monospace}
		.muted{color:#6b7280}
		.spinner{border:4px solid rgba(0,0,0,.1);border-left-color:#4f46e5;border-radius:50%;width:20px;height:20px;animation:spin .8s linear infinite;display:inline-block}
		@keyframes spin{to{transform:rotate(360deg)}}
		.kpi{display:flex;gap:10px;flex-wrap:wrap}
		.kpi div{background:#f9fafb;border:1px solid #e5e7eb;border-radius:10px;padding:8px 10px}
		.chip{display:inline-block;background:#eef2ff;border:1px solid #e5e7eb;border-radius:999px;padding:2px 8px;margin-right:6px;font-size:12px}
	</style>
</head>
<body>
<div class="container">
	<h1 class="text-2xl font-extrabold mb-1">PhO-Compress Atom-LLM — Linear Attention (O(M))</h1>
	<div class="muted mb-4">Laut-Atome (gelernte IPA-Subunits) → stark komprimierte Sequenz → Decoder-Transformer mit <b>kausaler Linear-Attention</b> (O(M)) in tf.js/WebGPU. Prototyp von <b>Christian Heinrich Hohlfeld</b>.</div>

	<!-- 1) Daten & Atomizer -->
	<div class="card">
		<h3 class="font-bold text-lg mb-2">1) Korpus & Atomizer (IPA → Atome per BPE)</h3>
		<textarea id="txt" placeholder="Füge Text ein (Deutsch/Englisch gemischt möglich). Mehr ist besser."></textarea>

		<div class="row" style="margin-top:10px">
			<div>
				<label>Atom-Vokabular (Ziel | inkl. Sondertoken)</label>
				<input id="atomVocab" type="number" min="64" max="1024" step="32" value="512"/>
			</div>
			<div>
				<label>Max. Kontext T (LLM)</label>
				<input id="seqLen" type="number" min="64" max="768" step="32" value="128"/>
			</div>
		</div>

		<div class="row" style="margin-top:10px">
			<div>
				<button id="btnBuildIPA" class="btn grey">1. G2P → IPA erzeugen</button>
			</div>
			<div>
				<button id="btnTrainAtoms" class="btn warn" disabled>2. Atome lernen (BPE auf IPA)</button>
			</div>
		</div>
		<div class="kpi mt-3">
			<div id="kpiIPA" class="muted">IPA: –</div>
			<div id="kpiAtoms" class="muted">Atome: –</div>
			<div id="kpiComp" class="muted">Kompression: –</div>
		</div>
		<div class="mt-3">
			<div class="chip">BOS</div><div class="chip">EOS</div><span class="muted">Sondertoken werden automatisch hinzugefügt.</span>
		</div>
	</div>

	<!-- 2) Modell & Training -->
	<div class="card">
		<h3 class="font-bold text-lg mb-2">2) LLM-Setup & Training (Decoder-Transformer mit Linear-Attention)</h3>
		<div class="row">
			<div>
				<label>Epochen</label>
				<input id="epochs" type="number" min="1" max="40" value="4"/>
			</div>
			<div>
				<label>LR (Adam)</label>
				<input id="lr" type="number" min="0.00005" max="0.02" step="0.00005" value="0.001"/>
			</div>
		</div>
		<div class="row" style="margin-top:10px">
			<div>
				<label>Batchgröße</label>
				<input id="batch" type="number" min="8" max="128" step="8" value="32"/>
			</div>
			<div>
				<label>Steps/Epoche (0=auto)</label>
				<input id="stepCap" type="number" min="0" max="50000" step="100" value="400"/>
			</div>
		</div>

		<div class="row" style="margin-top:10px">
			<div>
				<label>Schichten L</label>
				<input id="nLayers" type="number" min="1" max="8" value="3"/>
			</div>
			<div>
				<label>d_model</label>
				<input id="dModel" type="number" min="64" max="512" step="32" value="256"/>
			</div>
			<div>
				<label>Heads</label>
				<input id="nHeads" type="number" min="1" max="8" value="4"/>
			</div>
			<div>
				<label>FFN-Dim</label>
				<input id="dFF" type="number" min="128" max="2048" step="64" value="512"/>
			</div>
		</div>

		<div class="mt-2 flex flex-wrap gap-3 items-center">
			<label class="muted"><input id="useGPU" type="checkbox" checked/> WebGPU bevorzugen</label>
			<label class="muted"><input id="weightTying" type="checkbox" checked/> Weight-Tying</label>
			<label class="muted"><input id="useDropout" type="checkbox"/> Dropout 0.1</label>
			<label class="muted"><input id="autoLR" type="checkbox" checked/> Auto-LR</label>
		</div>

		<div class="flex items-center gap-3 mt-3">
			<button id="btnTrain" class="btn" disabled><span id="trainText">3. Training starten</span> <span id="spin" class="spinner" style="display:none;margin-left:8px"></span></button>
			<div id="stats" class="muted"></div>
		</div>
	</div>

	<!-- 3) Inferenz -->
	<div class="card">
		<h3 class="font-bold text-lg mb-2">3) Inferenz (Prompt → Ausgabe)</h3>
		<textarea id="prompt" placeholder="Prompt (Text) eingeben …" disabled></textarea>
		<div class="row">
			<div>
				<label>Sampling-Modus</label>
				<select id="sampleMode">
					<option value="topp" selected>Top-p</option>
					<option value="topk">Top-k</option>
				</select>
			</div>
			<div>
				<label>Top-k</label>
				<input id="topK" type="number" min="1" max="200" value="64" disabled/>
			</div>
			<div>
				<label>Top-p</label>
				<input id="topP" type="number" min="0.05" max="1.0" step="0.05" value="0.9"/>
			</div>
			<div>
				<label>Temperatur</label>
				<input id="temp" type="number" min="0.1" max="2.0" step="0.1" value="0.9"/>
			</div>
		</div>
		<div class="row mt-2">
			<div>
				<label>Tokens generieren</label>
				<input id="nGen" type="number" min="1" max="2000" value="200"/>
			</div>
			<div class="flex items-end gap-3">
				<button id="btnGen" class="btn secondary" disabled>Generieren</button>
				<div id="inferStats" class="muted"></div>
			</div>
		</div>
		<div class="mt-3">
			<div class="muted mb-1">Ausgabe</div>
			<div id="out" class="mono" style="white-space:pre-wrap;background:#f9fafb;border:1px solid #e5e7eb;border-radius:10px;padding:10px;min-height:80px"></div>
			<div id="outIPA" class="mono muted mt-1" style="white-space:pre-wrap"></div>
		</div>
	</div>

	<!-- 4) Metriken & Log -->
	<div class="card">
		<h3 class="font-bold text-lg mb-2">Perplexity & Log</h3>
		<div id="metrics" class="muted mb-2"></div>
		<div id="log" class="log">[Log bereit]</div>
	</div>
</div>

<script>
/* =================== Helpers & UI =================== */
const $ = id => document.getElementById(id);
const MAX_LOG_CHARS = 100000;

function logLine(msg){
	const el = $('log');
	const line = `[${new Date().toLocaleTimeString()}] ${msg}\n`;
	el.textContent = line + el.textContent;
	if (el.textContent.length > MAX_LOG_CHARS) {
		el.textContent = el.textContent.slice(0, MAX_LOG_CHARS);
	}
}
function clamp(v, lo, hi){
	v = Number.isFinite(v) ? v : lo;
	return Math.max(lo, Math.min(hi, v));
}
function valNum(id, fallback, lo=null, hi=null){
	const el = $(id);
	let v = Number(el?.value);
	if(!Number.isFinite(v)) v = fallback;
	if(lo!=null) v = Math.max(lo, v);
	if(hi!=null) v = Math.min(hi, v);
	return v;
}
function valBool(id, fallback=false){
	const el = $(id);
	return el?.checked ?? fallback;
}

/* =================== Text ⇄ IPA =================== */
// Wichtig: längste Sequenzen zuerst (kein „tsch“ -> „sch“ + „ch“-Zerfall)
function g2pConvert(text){
	if(!text) return '';
	const words = (text.normalize('NFC').toLowerCase().match(/[a-zäöüß]+|[a-z]+|[0-9]+|[^\s]/gi) || []).map(w=>w.toString());

	const repl = [
		// Längste Grapheme zuerst
		[/tsch/g,'t͡ʃ'],
		[/sch/g,'ʃ'],
		[/ch/g,'ç'],
		[/ts/g,'t͡s'],
		[/th/g,'t'], [/ph/g,'f'], [/qu/g,'kv'],
		[/ng/g,'ŋ'], [/nk/g,'ŋk'],
		// Vokale/Diphthonge
		[/eau/g,'oː'], [/eou/g,'uː'],
		[/aʊ/g,'aʊ'], // Platzhalter, falls schon IPA-ähnlich
		[/au/g,'aʊ̯'], [/ei/g,'aɪ̯'], [/ai/g,'aɪ̯'], [/eu/g,'ɔɪ̯'], [/äu/g,'ɔɪ̯'], [/ie/g,'iː'],
		// Basiskonsonanten
		[/z/g,'t͡s'], [/v/g,'f'], [/w/g,'v'], [/j/g,'j'], [/x/g,'ks'],
		// Umlaute
		[/ä/g,'ɛ'], [/ö/g,'œ'], [/ü/g,'y'],
		// Basis-Vokale (vereinfachend)
		[/a/g,'a'], [/e/g,'ə'], [/i/g,'ɪ'], [/o/g,'ɔ'], [/u/g,'ʊ'],
		// ß
		[/ß/g,'s']
	];

	const vowelLike = /[aɛəɪiɔoʊuœyː]/; // grob
	const ipaWords = [];

	for (let w of words){
		if (!/[a-zäöüß]/i.test(w)) { ipaWords.push(w); continue; }
		let s = w;
		for (const [rg, to] of repl) s = s.replace(rg, to);
		// Marker für primäre Betonung (ˈ) vor ersten vokalischen Laut
		let idx = s.search(vowelLike);
		if (idx >= 0) s = s.slice(0, idx) + 'ˈ' + s.slice(idx);
		s = s.replace(/ˈˈ+/g,'ˈ');
		ipaWords.push(s);
	}
	return ipaWords.join(' ').replace(/\s+/g,' ').trim();
}

function ipa2text(ipa){
	if(!ipa) return '';
	const table = [
		[/t͡ʃ/g,'tsch'], [/t͡s/g,'z'], [/ʃ/g,'sch'], [/ç/g,'ch'],
		[/ŋk/g,'nk'], [/ŋ/g,'ng'],
		[/aʊ̯/g,'au'], [/aɪ̯/g,'ei'], [/ɔɪ̯/g,'eu'],
		[/iː/g,'ie'], [/oː/g,'o'], [/uː/g,'u'],
		[/ɛ/g,'ä'], [/œ/g,'ö'], [/y/g,'ü'],
		[/ɪ/g,'i'], [/ʊ/g,'u'], [/ɔ/g,'o'], [/ə/g,'e'],
		[/ˈ/g,'']
	];
	let s = ipa;
	for (const [rg,to] of table) s = s.replace(rg,to);
	s = s.replace(/[ː]/g,'').replace(/\s+/g,' ').trim();
	return s;
}

/* =================== IPA-BPE Atomizer =================== */

const BOS_ID = 0, EOS_ID = 1; // reserviert

function strToCodepoints(str){
	const arr = [];
	for (let i=0;i<str.length;i++){
		arr.push(str.charCodeAt(i));
	}
	return arr;
}
function codepointsToStr(arr){
	return String.fromCharCode(...arr);
}

function buildAtomBPE(rawIPA, targetVocab=512){
	// Basis-Vokabular: alle Codepoints, in stabilem Mapping, ab ID=2
	const cps = strToCodepoints(rawIPA);
	const uniq = Array.from(new Set(cps.filter(x=>!Number.isNaN(x))));
	uniq.sort((a,b)=>a-b);
	const cp2id = new Map(); const id2cp = new Map();
	let nextId = 2;
	for (const c of uniq){ cp2id.set(c,nextId); id2cp.set(nextId,c); nextId++; }

	// Start-Sequenz als IDs
	let seq = cps.map(c => cp2id.get(c) ?? BOS_ID);

	// BPE Merges: {a,b,id}
	const merges = [];
	const baseVocab = nextId;

	function countPairs(sequence){
		const counts = new Map();
		for (let i=0;i<sequence.length-1;i++){
			const key = sequence[i] + ',' + sequence[i+1];
			counts.set(key, (counts.get(key)||0) + 1);
		}
		return counts;
	}
	function applyBestPair(sequence, a, b, newId){
		const out = [];
		for (let i=0;i<sequence.length;){
			if (i<sequence.length-1 && sequence[i]===a && sequence[i+1]===b){
				out.push(newId); i+=2;
			} else { out.push(sequence[i]); i+=1; }
		}
		return out;
	}

	const target = Math.max(baseVocab+10, Math.min(targetVocab, 4096));
	for (let step=0; step<(target - baseVocab); step++){
		const counts = countPairs(seq);
		if (counts.size===0) break;
		let bestKey=null, bestCnt=0;
		for (const [k,c] of counts){ if (c>bestCnt){ bestCnt=c; bestKey=k; } }
		if (!bestKey) break;
		const [aStr,bStr] = bestKey.split(',');
		const a=parseInt(aStr,10), b=parseInt(bStr,10);
		const id = nextId++;
		merges.push({a,b,id});
		seq = applyBestPair(seq, a, b, id);
		if ((step % 128) === 0) logLine(`BPE merge ${step+1}: (${a},${b}) → ${id} (count=${bestCnt})`);
	}

	function encodeIPAtoAtoms(str, addBos=false, addEos=false){
		const seq0 = strToCodepoints(str).map(c => cp2id.get(c));
		let tokens = seq0.filter(v=>Number.isFinite(v));
		for (const m of merges){
			const {a,b,id} = m;
			const out = [];
			for (let i=0;i<tokens.length;){
				if (i<tokens.length-1 && tokens[i]===a && tokens[i+1]===b){ out.push(id); i+=2; }
				else { out.push(tokens[i]); i++; }
			}
			tokens = out;
		}
		if (addBos) tokens = [BOS_ID, ...tokens];
		if (addEos) tokens = [...tokens, EOS_ID];
		return tokens;
	}
	function decodeAtomsToIPA(tokens){
		const id2pair = new Map(merges.map(m=>[m.id,[m.a,m.b]]));
		function expand(id){
			if (id===BOS_ID || id===EOS_ID) return [];
			if (id2cp.has(id)) return [ id2cp.get(id) ];
			const pr = id2pair.get(id);
			if (!pr) return [];
			return expand(pr[0]).concat(expand(pr[1]]);
		}
		const cpsOut = [];
		for (const t of tokens){
			if (t===BOS_ID || t===EOS_ID) continue;
			if (id2cp.has(t)) cpsOut.push(id2cp.get(t));
			else cpsOut.push(...expand(t));
		}
		return codepointsToStr(cpsOut);
	}

	const vocabSize = nextId;
	const api = { encodeIPAtoAtoms, decodeAtomsToIPA, vocabSize, merges, cp2id, id2cp };
	return api;
}

/* =================== Dataset Builder =================== */
function buildStarts(tokens, T, valPct){
	const N = tokens.length;
	logLine(`buildStarts: N=${N}, T=${T}, valPct=${valPct}`);
	const usable = Math.max(0, N - (T+1));
	const all = new Uint32Array(usable);
	for (let i=0;i<usable;i++) all[i]=i;
	const valN = Math.floor(usable*(valPct/100));
	const trainN = usable - valN;
	return {train: all.slice(0,trainN), val: all.slice(trainN)};
}
function sampleBatch(starts, tokens, B, T){
	const X = new Int32Array(B*T), Y = new Int32Array(B*T);
	const n = starts.length;
	if (n===0) return {X,Y};
	for (let b=0;b<B;b++){
		const s = starts[(Math.random()*n)|0];
		for (let t=0;t<T;t++){
			X[b*T+t] = tokens[s+t];
			Y[b*T+t] = tokens[s+t+1];
		}
	}
	return {X,Y};
}

/* =================== Transformer (O(M) Linear-Attention) =================== */
function rmsNorm(x, gamma, eps=1e-5){
	const meanSq = tf.mean(tf.square(x), -1, true);
	const xhat = tf.div(x, tf.sqrt(tf.add(meanSq, eps)));
	return tf.mul(xhat, gamma);
}
function sinusoidalPositionalEncoding(T, d){
	const pos = tf.range(0, T, 1, 'float32');
	const half = Math.floor(d/2);
	const i2 = tf.range(0, half, 1, 'float32');
	const div = tf.exp(tf.mul(i2, tf.scalar(-Math.log(10000.0)/(half-1))));
	const ang = tf.outerProduct(pos, div);
	const sin = tf.sin(ang), cos=tf.cos(ang);
	let pe = tf.concat([sin, cos], -1);
	if(2*half < d){
		const pad = tf.zeros([T, d-2*half]);
		pe = tf.concat([pe, pad], -1);
	}
	return pe;
}
function splitHeads(x, nHeads){
	const [B,T,d] = x.shape;
	const dh = d/nHeads;
	return x.reshape([B,T,nHeads,dh]).transpose([0,2,1,3]);
}
function combineHeads(x){
	const [B,nH,T,dh] = x.shape;
	return x.transpose([0,2,1,3]).reshape([B,T,nH*dh]);
}

// Kernel-Feature: φ(x) = ELU(x) + 1  (immer positiv)
function phi(x){ return tf.elu(x).add(1); }

/**
 * Kausale Linear-Attention (FAVOR/Kernel-Trick) — O(T) pro Head und Batch
 * Expect q,k,v in shape [B, H, T, D]
 */
function linearCausalAttention(q, k, v, eps=1e-6){
	const B = q.shape[0], H = q.shape[1], T = q.shape[2], D = q.shape[3];
	let Ksum  = tf.zeros([B, H, D, 1]);
	let KVsum = tf.zeros([B, H, D, D]);
	const outs = [];

	for (let t = 0; t < T; t++){
		const res = tf.tidy(() => {
			const qt = q.slice([0,0,t,0],[B,H,1,D]);      // [B,H,1,D]
			const kt = k.slice([0,0,t,0],[B,H,1,D]);      // [B,H,1,D]
			const vt = v.slice([0,0,t,0],[B,H,1,D]);      // [B,H,1,D]

			const ktT = kt.reshape([B,H,D,1]);            // [B,H,D,1]
			const vtT = vt.reshape([B,H,1,D]);            // [B,H,1,D]

			const outer = tf.matMul(ktT, vtT);            // [B,H,D,D]
			const KVsumNew = KVsum.add(outer);            // ∑ kτ ⊗ vτ
			const KsumNew  = Ksum.add(ktT);               // ∑ kτ

			// y_t = (q_t · KVsum) / (q_t · Ksum)
			const num = tf.matMul(qt, KVsumNew);          // [B,H,1,D]
			const den = tf.matMul(qt, KsumNew).add(eps);  // [B,H,1,1]
			const yt  = tf.div(num, den);                 // [B,H,1,D]
			return {KVsumNew, KsumNew, yt};
		});
		// dispose alte Summen und übernehmen neue
		KVsum.dispose(); Ksum.dispose();
		KVsum = res.KVsumNew; Ksum = res.KsumNew;
		outs.push(res.yt);
	}
	const out = tf.tidy(()=> tf.concat(outs, 2)); // [B,H,T,D]
	outs.forEach(t => t.dispose());
	KVsum.dispose(); Ksum.dispose();
	return out;
}

function softmaxCEfromLogits(logits, targets){
	const [B,T,V] = logits.shape;
	const oneHot = tf.oneHot(targets.toInt(), V).reshape([B*T, V]);
	const logits2d = logits.reshape([B*T, V]);
	const lossPer = tf.losses.softmaxCrossEntropy(oneHot, logits2d);
	return tf.mean(lossPer);
}

function buildModel(cfg){
	const { V, dModel, nHeads, dFF, nLayers, T, weightTying, dropoutTrain } = cfg;

	const E = tf.variable(tf.randomNormal([V, dModel], 0, 0.02, 'float32'));
	const pe = sinusoidalPositionalEncoding(T, dModel);
	const layers = [];
	for(let l=0;l<nLayers;l++){
		layers.push({
			g1: tf.variable(tf.ones([dModel])),
			g2: tf.variable(tf.ones([dModel])),
			Wq: tf.variable(tf.randomNormal([dModel,dModel], 0, Math.sqrt(2/(dModel+dModel)))) ,
			Wk: tf.variable(tf.randomNormal([dModel,dModel], 0, Math.sqrt(2/(dModel+dModel)))) ,
			Wv: tf.variable(tf.randomNormal([dModel,dModel], 0, Math.sqrt(2/(dModel+dModel)))) ,
			Wo: tf.variable(tf.randomNormal([dModel,dModel], 0, Math.sqrt(2/(dModel+dModel)))) ,
			W1: tf.variable(tf.randomNormal([dModel,dFF]  , 0, Math.sqrt(2/(dModel+dFF)))) ,
			b1: tf.variable(tf.zeros([dFF])),
			W2: tf.variable(tf.randomNormal([dFF,dModel]  , 0, Math.sqrt(2/(dFF+dModel)))) ,
			b2: tf.variable(tf.zeros([dModel])),
		});
	}
	const bout = tf.variable(tf.zeros([V]));
	const W_out = weightTying ? null : tf.variable(tf.randomNormal([dModel, V], 0, 0.02, 'float32'));
	const params = {E, layers, bout, pe, cfg, W_out};

	function dropout(x, rate){
		if(!dropoutTrain || rate<=0) return x;
		const keep = 1-rate;
		const m = tf.randomUniform(x.shape,'float32').greater(tf.scalar(rate)).toFloat();
		return tf.mul(x, tf.div(m, tf.scalar(keep)));
	}

	function forward(ids, training=true){
		return tf.tidy(() => {
			const B = ids.shape[0];
			const Tcur = ids.shape[1];
			const dM = params.cfg.dModel;
			const nH = params.cfg.nHeads;
			const V = params.cfg.V;
			const BT = B * Tcur;

			// Embedding + Positional Encoding
			const emb = tf.gather(params.E, ids.flatten()).reshape([B,Tcur,dM]);
			let x = emb.add(params.pe.slice([0,0],[Tcur,dM]).reshape([1,Tcur,dM]));
			if (params.cfg.dropoutTrain && training) x = dropout(x, 0.1);

			for(const L of params.layers){
				// --- Linear-Attention Block (O(T)) ---
				let h = rmsNorm(x, L.g1);
				const h2d = h.reshape([BT, dM]);
				const Q = h2d.matMul(L.Wq).reshape([B,Tcur,dM]);
				const K = h2d.matMul(L.Wk).reshape([B,Tcur,dM]);
				const Vv= h2d.matMul(L.Wv).reshape([B,Tcur,dM]);

				let q = splitHeads(Q, nH); // [B,nH,T,dH]
				let k = splitHeads(K, nH);
				let v = splitHeads(Vv,nH);

				// Kernel-Feature (positiv)
				q = phi(q); k = phi(k);
				let ctx = linearCausalAttention(q, k, v); // [B,nH,T,dH]

				const attn2d = combineHeads(ctx).reshape([BT, dM]);
				const attnOut = attn2d.matMul(L.Wo).reshape([B, Tcur, dM]);
				x = x.add(dropout(attnOut, 0.1));

				// --- MLP ---
				let h2 = rmsNorm(x, L.g2);
				const h2_2d = h2.reshape([BT, dM]);
				let mlp2 = h2_2d.matMul(L.W1).add(L.b1);
				mlp2 = tf.mul(mlp2, tf.sigmoid(mlp2)); // SiLU
				mlp2 = mlp2.matMul(L.W2).add(L.b2);
				const mlp = mlp2.reshape([B, Tcur, dM]);
				x = x.add(dropout(mlp, 0.1));
			}

			// Output
			const x_2d = x.reshape([BT, dM]);
			let logits_2d;
			if (params.cfg.weightTying){
				logits_2d = x_2d.matMul(params.E.transpose());
			} else {
				logits_2d = x_2d.matMul(params.W_out);
			}
			const logits = logits_2d.reshape([B, Tcur, V]);
			return logits.add(params.bout);
		});
	}

	function variables(){
		const vs = [E, bout];
		for (const L of layers){
			vs.push(L.g1,L.g2,L.Wq,L.Wk,L.Wv,L.Wo,L.W1,L.b1,L.W2,L.b2);
		}
		if (params.W_out) vs.push(params.W_out);
		return vs;
	}
	return {params, forward, variables};
}

/* =================== Global State =================== */
let ATOMIZER=null;
let MODEL=null;
let TOKENS=null;
let IPA_CORPUS='';

const YIELD_EVERY = 128;
const LOG_EVERY   = 200;
const MEM_EVERY   = 512;

/* =================== Backend Setup =================== */
async function setupBackend(preferGPU=true){
	const canWebGPU = !!tf.engine().registryFactory['webgpu'];
	const canWebGL  = !!tf.engine().registryFactory['webgl'];

	try {
		if (preferGPU && canWebGPU) {
			await tf.setBackend('webgpu');
			await tf.ready();
			if (navigator?.gpu?.requestAdapter){
				const adapter = await navigator.gpu.requestAdapter();
				if (adapter && 'info' in adapter){
					const i = adapter.info; logLine(`WebGPU-Adapter: ${i.vendor ?? 'unknown'} ${i.architecture ?? ''}`);
				}
			}
			logLine(`Backend: ${tf.getBackend()} (Linear-Attention aktiv)`);
			return;
		}
		throw new Error('WebGPU factory not available');
	} catch(e){
		logLine(`WebGPU init fehlgeschlagen (${e?.message||e}). Fallback auf WebGL/CPU...`);
	}

	if (canWebGL) {
		await tf.setBackend('webgl'); await tf.ready(); logLine(`Backend: ${tf.getBackend()} (WebGL)`); return;
	}
	await tf.setBackend('cpu'); await tf.ready(); logLine(`Backend: ${tf.getBackend()} (CPU)`);
}

/* =================== Pipe: Text → IPA → Atome =================== */
function buildTokenDataFromText(rawText, T, atomVocab){
	// 1. G2P Konvertierung
	IPA_CORPUS = g2pConvert(rawText);
	const ipaLen = IPA_CORPUS.length;
	$('kpiIPA').textContent = `IPA: ${ipaLen} Zeichen`;
	$('btnTrainAtoms').disabled = false;

	// 2. Atomizer trainieren (BPE)
	logLine(`Trainiere Atomizer (BPE) auf IPA… Ziel-Vokabular=${atomVocab}`);
	ATOMIZER = buildAtomBPE(IPA_CORPUS, atomVocab);

	// 3. Korpus in Atome encodieren
	const atomIds = ATOMIZER.encodeIPAtoAtoms(IPA_CORPUS, true, true); // BOS/EOS für Training
	TOKENS = new Uint32Array(atomIds);
	const atomCount = TOKENS.length;

	$('kpiAtoms').textContent = `Atome: ${atomCount} (inkl. BOS/EOS)`;
	const roughComp = (ipaLen>0) ? (ipaLen / atomCount).toFixed(2) : '–';
	$('kpiComp').textContent = `Kompression (Zeichen→Atome): ${roughComp}×`;

	$('btnTrain').disabled = false;
	logLine(`Atomizer (BPE) trainiert & Korpus atomisiert. Vocab=${ATOMIZER.vocabSize}.`);
}

/* =================== Training =================== */
async function train(){
	const preferGPU = valBool('useGPU', true);
	await setupBackend(preferGPU);

	const raw = ($('txt')?.value ?? '');
	if(!raw.trim()){ logLine('Fehler: Kein Text.'); return; }
	if(!ATOMIZER || !TOKENS){ logLine('Fehler: Atomizer/Tokens fehlen. Bitte zuerst Atome lernen.'); return; }

	$('btnTrain').disabled = true; $('spin').style.display='inline-block'; $('trainText').textContent='Trainiere…';

	try {
		const T = valNum('seqLen', 128, 64, 768);
		const epochs   = clamp(valNum('epochs', 4, 1, 40), 1, 40);
		let lr         = clamp(valNum('lr', 0.001, 0.00005, 0.02), 0.00005, 0.02);
		const B        = clamp(valNum('batch', 32, 8, 128), 8, 128);
		const stepCap  = clamp(valNum('stepCap', 400, 0, 50000), 0, 50000);
		const valSplit = 10;
		const autoLR   = valBool('autoLR', true);

		const L        = clamp(valNum('nLayers', 3, 1, 8), 1, 8);
		const dModel   = clamp(valNum('dModel', 256, 64, 512), 64, 512);
		const nHeads   = clamp(valNum('nHeads', 4, 1, 8), 1, 8);
		const dFF      = clamp(valNum('dFF', 512, 128, 2048), 128, 2048);
		const weightTying = valBool('weightTying', true);
		const dropoutTrain = valBool('useDropout', false);
		const V = ATOMIZER.vocabSize;

		if (dModel % nHeads !== 0){
			logLine('Warnung: d_model nicht durch Heads teilbar. Bitte anpassen.'); 
		}

		if (MODEL) MODEL.variables().forEach(v => v.dispose());
		MODEL = buildModel({V, dModel, nHeads, dFF, nLayers: L, T, weightTying, dropoutTrain});
		const optim = tf.train.adam(lr, 0.9, 0.999, 1e-8);

		const {train:trainStarts, val:valStarts} = buildStarts(TOKENS, T, valSplit);
		const rawSteps = Math.max(1, Math.floor(trainStarts.length / B));
		const stepsPerEpoch = stepCap>0 ? Math.min(stepCap, rawSteps) : rawSteps;

		$('stats').textContent = `V=${V} • T=${T} • B=${B} • L=${L} • d=${dModel} • h=${nHeads} • dFF=${dFF} • Steps/Ep=${stepsPerEpoch} • Attention=Linear(O(M))`;
		logLine(`Training konfiguriert: Epochen=${epochs}, LR=${lr}, B=${B}, Steps/Ep=${stepsPerEpoch} (raw=${rawSteps})`);

		let bestPPL=Infinity, bad=0;

		for (let ep=0; ep<epochs; ep++){
			logLine(`Starte Epoche ${ep+1}/${epochs} mit LR=${lr.toFixed(6)}`);
			const t0 = performance.now();
			let lossAcc=0;

			for (let s=0;s<stepsPerEpoch;s++){
				const {X,Y} = sampleBatch(trainStarts, TOKENS, B, T);

				const lossVal = tf.tidy(() => {
					const x = tf.tensor2d(X, [B,T], 'int32');
					const y = tf.tensor2d(Y, [B,T], 'int32');

					const vars = MODEL.variables();
					const {value, grads} = tf.variableGrads(()=>{
						const logits = MODEL.forward(x, true);
						return softmaxCEfromLogits(logits, y);
					}, vars);

					// Gradient-Update
					const gradValues = Object.values(grads);
					optim.applyGradients(grads);
					gradValues.forEach(g=>g?.dispose());

					const v = value.dataSync()[0];
					value.dispose();
					return v;
				});

				if (!Number.isFinite(lossVal)) { logLine(`FEHLER: Loss=${lossVal}. Abbruch.`); throw new Error('NaN/Inf'); }
				lossAcc += lossVal;

				if ((s % LOG_EVERY) === 0) logLine(`Ep ${ep+1} Step ${s}/${stepsPerEpoch} Loss=${lossVal.toFixed(4)}`);
				if ((s % MEM_EVERY) === 0) { const m=tf.memory(); logLine(`mem: tensors=${m.numTensors} bytes=${m.numBytes}`); }
				if ((s % YIELD_EVERY) === 0) await tf.nextFrame();
			}

			const t1 = performance.now();
			logLine(`Epoche ${ep+1} Training beendet. Starte Evaluierung…`);

			// Kleine Val-Eval (optional)
			const evalB = Math.min(B, 32);
			const evalSteps = Math.min(10, Math.max(0, Math.floor(valStarts.length / Math.max(1, evalB))));
			let nll=0, tok=0;
			if (evalSteps>0){
				for (let es=0; es<evalSteps; es++){
					const {X,Y} = sampleBatch(valStarts, TOKENS, evalB, T);
					const v = tf.tidy(()=>{
						const x=tf.tensor2d(X,[evalB,T],'int32');
						const y=tf.tensor2d(Y,[evalB,T],'int32');
						const logits=MODEL.forward(x,false);
						const ce = softmaxCEfromLogits(logits,y);
						return ce.dataSync()[0];
					});
					nll += v*(evalB*T);
					tok += (evalB*T);
					if ((es & 3)===0) await tf.nextFrame();
				}
			}
			const ppl = Math.exp(nll/Math.max(1,tok));
			const tokCount = stepsPerEpoch * B * T;
			const tokps = tokCount / Math.max(0.001, (t1-t0)/1000);
			$('metrics').innerHTML = `<b>Epoche ${ep+1}/${epochs}</b> &nbsp; Loss=${(lossAcc/stepsPerEpoch).toFixed(4)} &nbsp; PPL=${Number.isFinite(ppl)?ppl.toFixed(2):'—'} &nbsp; ~Tok/s=${tokps.toFixed(0)} &nbsp; LR=${lr.toFixed(5)}`;
			logLine(`Ep ${ep+1}: loss=${(lossAcc/stepsPerEpoch).toFixed(4)} | PPL=${Number.isFinite(ppl)?ppl.toFixed(2):'—'} | ${(t1-t0).toFixed(0)}ms | ~${tokps.toFixed(0)} tok/s`);

			if (autoLR){
				if (ppl + 0.01 < bestPPL){ bestPPL=ppl; bad=0; }
				else { bad++; if (bad>=2 && lr>1e-4){ lr = Math.max(1e-4, lr*0.5); optim.setLearningRate(lr); bad=0; logLine(`Auto-LR → ${lr.toFixed(6)}`);} }
			}
			await tf.nextFrame();
		}
		logLine('Training abgeschlossen.');
		$('prompt').disabled = false;
		$('btnGen').disabled = false;
	} catch(e){
		console.error(e); logLine('Fehler: '+(e?.message||e));
	} finally {
		$('btnTrain').disabled = false; $('spin').style.display='none'; $('trainText').textContent='3. Training starten';
	}
}

/* =================== Inferenz =================== */
function sampleFromProbs(p, mode, topK, topP, temp){
	const t = clamp(temp, 0.1, 2.0);
	const logp = p.map(x=>Math.log(x+1e-20)/t);
	const maxv = Math.max(...logp);
	const probs = logp.map(v=>Math.exp(v-maxv));
	let Z = probs.reduce((a,b)=>a+b,0); for(let i=0;i<probs.length;i++) probs[i]/=Z;
	const order = Array.from(probs.keys()).sort((a,b)=>probs[b]-probs[a]);
	if(mode==='topk'){
		const K = Math.max(1, Math.min(topK, probs.length));
		const keep = order.slice(0,K);
		let s=0; for(const i of keep) s+=probs[i];
		let r=Math.random()*s;
		for(const i of keep){ r-=probs[i]; if(r<=0) return i; }
		return keep[keep.length-1];
	} else {
		const pth = clamp(topP, 0.05, 1.0);
		let cum=0, keep=[];
		for(const i of order){ keep.push(i); cum+=probs[i]; if(cum>=pth) break; }
		let s=0; for(const i of keep) s+=probs[i];
		let r=Math.random()*s;
		for(const i of keep){ r-=probs[i]; if(r<=0) return i; }
		return keep[keep.length-1];
	}
}

async function generate(){
	if (!MODEL || !ATOMIZER){ logLine('Bitte zuerst Atomizer + Modell trainieren.'); return; }
	const mode = $('sampleMode').value;
	const topK = valNum('topK', 64, 1, 200);
	const topP = valNum('topP', 0.9, 0.05, 1.0);
	const temp = valNum('temp', 0.9, 0.1, 2.0);
	const nGen = valNum('nGen', 200, 1, 2000);
	const T = MODEL.params.cfg.T;

	const prompt = ($('prompt')?.value ?? '').trim();
	if (!prompt){ logLine('Kein Prompt.'); return; }

	$('btnGen').disabled = true; $('inferStats').textContent = 'Generiere...';

	const promptIPA = g2pConvert(prompt);
	let ctx = ATOMIZER.encodeIPAtoAtoms(promptIPA, true, false); // BOS + Prompt
	const t0 = performance.now();

	let outTokens = [];
	for(let step=0; step<nGen; step++){
		const ctxSlice = ctx.slice(-T);
		const B=1, len=ctxSlice.length;
		const padLen = T - len;
		const inp = (padLen>0) ? (new Int32Array([...Array(padLen).fill(BOS_ID), ...ctxSlice])) : new Int32Array(ctxSlice);

		const last = tf.tidy(()=>{
			const x=tf.tensor2d(inp,[B,T],'int32');
			const logits = MODEL.forward(x,false);
			const lastLogits = logits.slice([0,T-1,0],[1,1,MODEL.params.cfg.V]);
			return lastLogits.arraySync()[0][0];
		});

		const maxv = Math.max(...last);
		const probs = last.map(v=>Math.exp(v-maxv));
		let Z = probs.reduce((a,b)=>a+b,0); for(let i=0;i<probs.length;i++) probs[i]/=Z;

		const nxt = sampleFromProbs(probs, mode, topK, topP, temp);
		if (nxt === EOS_ID) break;

		ctx.push(nxt); outTokens.push(nxt);
		if((step & 31)===0) await tf.nextFrame();
	}
	const t1 = performance.now();
	$('btnGen').disabled = false;
	$('inferStats').textContent = `Tokens: ${outTokens.length} • Latenz ${(t1-t0).toFixed(1)} ms`;

	const genIPA = ATOMIZER.decodeAtomsToIPA(outTokens);
	const genText = ipa2text(genIPA);
	$('out').textContent = genText;
	$('outIPA').textContent = `IPA: ${genIPA}`;
	logLine(`Generiert ${outTokens.length} Tokens (${(t1-t0).toFixed(0)} ms).`);
}

/* =================== Wire UI =================== */
window.addEventListener('load', ()=>{
	$('btnBuildIPA').addEventListener('click', ()=>{
	 const raw=($('txt')?.value??'').trim();
	 if(!raw){ logLine('Kein Text.'); return; }
	 const ipa=g2pConvert(raw);
	 IPA_CORPUS = ipa;
	 $('kpiIPA').textContent = `IPA: ${ipa.length} Zeichen`;
	 $('btnTrainAtoms').disabled = false;
	 logLine('G2P → IPA fertig.');
	});

	$('btnTrainAtoms').addEventListener('click', ()=>{
	 const raw=($('txt')?.value??'').trim();
	 if(!raw){ logLine('Kein Text.'); return; }
	 const atomVocab = clamp(valNum('atomVocab', 512, 64, 1024), 64, 4096);
	 const T        = clamp(valNum('seqLen',   128, 64, 768), 64, 768);

	 $('btnTrainAtoms').disabled = true; $('btnBuildIPA').disabled = true;
	 $('inferStats').textContent = 'Atomizer wird trainiert...';

	 setTimeout(() => {
		try {
			buildTokenDataFromText(raw, T, atomVocab);
			$('btnTrain').disabled = false;
			$('inferStats').textContent = 'Atome fertig. Bereit zum Training.';
		} catch (e) {
			logLine(`Fehler beim Atomizer-Training: ${e.message}`);
		} finally {
			$('btnTrainAtoms').disabled = false; $('btnBuildIPA').disabled = false;
		}
	 }, 10);
	});

	$('btnTrain').addEventListener('click', train);
	$('btnGen').addEventListener('click', generate);

	$('sampleMode').addEventListener('change', ()=>{
		const m=$('sampleMode').value;
		$('topK').disabled = (m!=='topk'); $('topP').disabled=(m!=='topp');
	});

	// Backend initialisieren
	setupBackend(valBool('useGPU', true));
});
</script>
</body>
</html>
